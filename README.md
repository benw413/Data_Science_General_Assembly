## Data Science Immersive Projects

Hello World. This repository contains multiple projects that I am either currently working on or have completed for my Data Science Immersive course.
The first project I have posted in this repository is a series of three separate Jupyter notebooks containing the a progression from basic data analysis to professional level workflow with machine learning on the kaggle.com Titanic dataset. The purpose for these notebooks are to serve as a tutorial for how to progress from the basics of descriptive analytics to the more advanced techniques of machine learning and predictive analytics. Over three separate Jupyter notebooks, I cover a step by step breakdown of how to explore, clean, analyze, visualize, and predict survival for Titanic passengers. I will demonstrate everything from importing libraries to the nuances of Kaggle including understanding and navigating the train and test csv environments, why you need to use both, and how to convert your results into a format that Kaggle will accept. Finally, I will also demonstrate how to automate workflow using pipelines to make code reproducible.

If you would like to learn more info about my Data Science Experience please feel free to check out my Blog at


#### Series 1: Data Analysis with the Titanic
This notebook uses pandas to explore, clean, analyze, and visualize basic information about the Titanic dataset and includes findings related to survival based on age, gender, class, and a host of other things.

#### Series 2: Logistic Regression w/out pipelines and how to upload predictions to Kaggle
This is a step by step guide to cleaning a dataset, manipulating the features, passing the model into a logistic regression function, obtaining predictions, and passing the dataset into a pandas dataframe to upload to Kaggle. As you will see, my process is very step by step to help with visualization but not very reproducible. I will explain how to reproduce this in the next series with pipelines.

#### Series 3: Logistic Regression with pipelines: How to automate workflow.
In this third and final jupyter notebook I will demonstrate how to use pipelines to allow for workflow to be reproducible. I will also upload this final dataset to Kaggle.com


I will continue to add more content to this ReadMe file as I continue to create new projects.  
